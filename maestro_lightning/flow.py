__all__ = [
    "Flow",
    "Session",
    "dump",
    "load",
    "print_datasets",
    "print_images",
    "print_tasks",
]

import os
import json
import tempfile

from loguru import logger
from tabulate import tabulate
from typing import Dict
from maestro_lightning.models import Context, Dataset, Image, Task, State
from maestro_lightning import get_context, get_hash, setup_logs



class Flow:

    def __init__(self, 
                 name           : str = "local",
                 virtualenv     : str,
                 path           : str = f"{os.getcwd()}/tasks",
                 level          : str="INFO",
                 partition_for_trigger : str="cpu",
        ):
            """
            Initializes a new instance of the class.

            Parameters:
            ----------
            name : str, optional
                The name of the provider. Defaults to "local".
            path : str, optional
                The file path where tasks are located. Defaults to the current working directory followed by '/tasks'.
            virtualenv : str, optional
                The path to the virtual environment. Defaults to the value of the environment variable 'VIRTUAL_ENV'.

            Attributes:
            ----------
            name : str
                The name of the provider.
            path : str
                The file path where tasks are located.
            virtualenv : str
                The path to the virtual environment.
            """
            
            self.name = name
            self.path = path
            self.extra_params = {
                "virtualenv": virtualenv,
                "partition_for_trigger": partition_for_trigger
            }
            setup_logs( name = f"Flow:{self.name}", level=level )
        
    def __enter__(self):
        return Session( self.path , extra_params = self.extra_params)

    def __exit__(self, exc_type, exc_value, traceback):
        pass


class Session:

    def __init__(self, path: str, extra_params : Dict[str,str]):
        self.path = path
        ctx = get_context(clear=True)
        ctx.path = path
        # decorate context with extra config
        for key, value in extra_params.items():
            ctx[key]=value
    
    def mkdir(self):
        logger.info(f"Creating flow directory at {self.path}")
        os.makedirs(self.path + "/tasks", exist_ok=True)
        os.makedirs(self.path + "/datasets", exist_ok=True)
        os.makedirs(self.path + "/images", exist_ok=True)

    def run(self, dry_run : bool=False):
        ctx = get_context()
        logger.info(f"Running flow at {self.path}")
        if not os.path.exists(f"{self.path}/flow.json"):
            logger.info("No existing tasks found, initializing new flow.")
            self.mkdir()
            # Save tasks to disk
            dump( ctx, f"{self.path}/flow.json" )
            logger.info(f"Tasks saved to {self.path}/flow.json")
            [image.mkdir() for image in ctx.images.values()]
            [dataset.mkdir() for dataset in ctx.datasets.values()]
            [task.mkdir() for task in ctx.tasks.values()]
            # Execute tasks with no dependencies as entry points
            for task in ctx.tasks.values():
                if len(task.prev) == 0:
                    logger.info(f"Preparing task {task.name} for execution.")
                    command = f"maestro run task -t {self.path}/flow.json -i {task.task_id}"
                    command+=" --dry-run" if dry_run else ""
                    print(command)
                    os.system(command)
            
          
        else:
            # Create a temporary file
            logger.info("Existing tasks found, verifying integrity before execution.")
            with tempfile.NamedTemporaryFile(delete=True) as temp_file:
                temp_file_path = temp_file.name
                dump(ctx, temp_file_path)
                temp_hash = get_hash(temp_file_path)
            original_hash = get_hash(f"{self.path}/flow.json")

            # Compare hashes
            if original_hash != temp_hash:
                raise Exception("Tasks have changed, you can not proceed with execution. Please create a new Flow instance or delete the current flow directory or rename it.")
            else:
                logger.info("No changes detected in tasks.")
            logger.info(f"Executing tasks in flow located at {self.path}.")
            #self.print_tasks()
        
        print("ðŸš¨ Please do not remove or move the flow directory or any dataset paths!\n"
              "ðŸš¨ Any changes may break the program and lead to unexpected behavior.")
           
        self.print()
            
    def print(self):
        print_images( get_context() )
        print_datasets( get_context() )
        print_tasks( get_context() )
              
#
# read and write functions
#
        
def dump( ctx : Context, path : str):
    
    with open(path, 'w') as f:
        d = {
            "datasets":{},
            "images":{},
            "tasks":{},
            "path":ctx.path,
            "extra_params": ctx.extra_params
        }
        # step 1: dump all datasets which are not from tasks
        for dataset in ctx.datasets.values():
            if not dataset.from_task:
                d['datasets'][ dataset.name ] = dataset.to_dict()
        # step 2: dump all images
        for images in ctx.images.values():
            d['images'][ images.name ] = images.to_dict()  
        # step 3: dump all tasks
        for task in ctx.tasks.values():
            d[ 'tasks' ][ task.task_id ] = task.to_dict()
        json.dump( d , f , indent=2 )
    
def load( path : str, ctx : Context):
    
    with open( path , 'r') as f:
        data = json.load(f)
        ctx.path = data['path']
        ctx.extra_params = data['extra_params']
        # step 1: load all datasets which are not from tasks
        for dataset in data['datasets'].values():
            Dataset.from_dict( dataset )
        # step 2: load all images
        for image in data['images'].values():
            Image.from_dict( image )
        # step 3: load all tasks
        for task in data['tasks'].values():
            Task.from_dict( task )
   
def print_datasets( ctx : Context): 
    logger.info("Current datasets in the flow:")       
    rows  = []
    for dataset in ctx.datasets.values():
        row = [dataset.name, len(dataset)]
        rows.append(row)
    cols = ['dataset', 'num_files']
    table = tabulate(rows ,headers=cols, tablefmt="psql")
    print(table)   
        
def print_images( ctx : Context):
    logger.info("Current images in the flow:")
    ctx = get_context()
    rows  = []
    for image in ctx.images.values():
        row = [image.name, image.path]
        rows.append(row)
    cols = ['image', 'path']
    table = tabulate(rows ,headers=cols, tablefmt="psql")
    print(table)   
        
def print_tasks(ctx : Context):
    logger.info("Current tasks in the flow:")
    rows  = []
    for task in ctx.tasks.values():
        row = [task.name, task.task_id]
        count = task.count()
        row.extend( [value for value in count.values()])
        row.extend([task.status.value])
        rows.append(row)
    cols = ['taskname','task_id']
    cols.extend([name for name in count.keys()])
    cols.extend(["status"])
    table = tabulate(rows ,headers=cols, tablefmt="psql")
    print(table)
    
         