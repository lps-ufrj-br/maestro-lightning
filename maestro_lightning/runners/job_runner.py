__all__ = []

import json
import argparse
import traceback
import shutil
import os, sys

from time import sleep
from loguru import logger
from pprint import pprint
from loguru import logger
from maestro_lightning import setup_logs, Popen, symlink
from maestro_lightning import Job, State


         
         
def run_job( args ):

    setup_logs( name = f"job_runner", level=args.message_level )
    workarea = args.output

    logger.info(f"loaded job from input file {args.input}.")
    with open ( args.input , 'r') as f:    
        job = Job.from_dict( json.load(f) )
        
    logger.info(f"job id: {job.job_id}")
    logger.info(f"reset job status...")
    job.reset()
    job.status = State.PENDING
    
    command = job.command
    job_id = job.job_id

    logger.info("starting...")
    os.makedirs( workarea, exist_ok=True)
    logger.info(f"workarea {workarea} created.")
    logger.info("preparing command...")
    print(command)
    
     
    logger.info(f"starting env builder for job {job_id}...")
    logger.info(f"workarea {workarea}...")

    if job.image:
        logger.info("preparing singularity image...")
        image     = job.image.path
        imagename = image.split('/')[-1]
        logger.info(f"using singularity image with name {imagename}.")
        linkpath  = symlink(image, f"{workarea}/{imagename}")
        image     = linkpath
        logger.info(f"singularity image linked to workarea at {linkpath}.")
    
    logger.info("creating secondary data links inside of the job workarea...")
    for key, dataset in job.secondary_data.items():
        logger.info(f"creating secondary data link for {dataset.name} inside of the job workarea.")
        linkpath = symlink( dataset.path , f"{workarea}/{dataset.name}")
        command = command.replace(f"%{key}", linkpath)

    
    logger.info("creating input data link inside of the job workarea...")
    input_data = job.input_file
    filename = input_data.split('/')[-1]
    dataset_name = input_data.split('/')[-2]
    linkpath = symlink( input_data, f"{workarea}/{dataset_name}.{filename}")
    command = command.replace(f"%IN", linkpath)
      
    logger.info("preparing output data locations...")
    outputs = []
    outputs_data = job.outputs
    for key, (filename, dataset) in outputs_data.items():
        logger.info(f"preparing output file {filename} for dataset {dataset.name}...")
        targetpath = dataset.path
        filename, extension = os.path.splitext( filename )
        filename = f"{filename}.{job_id}{extension}"
        targetpath = f"{targetpath}/{filename}"
        sourcepath = f"{workarea}/{filename}"
        command = command.replace(f"%{key}", sourcepath)
        outputs.append( (sourcepath, targetpath) )
        
    entrypoint = f"{workarea}/entrypoint.sh"
    with open(entrypoint,'w') as f:
        f.write(f"cd {workarea}\n")
        f.write(command)
            
    try:
        logger.info(f"entrypoint script created at {entrypoint}.")
        if job.image:
            logger.info("preparing singularity command...")
            binds   = f''
            for key,value in job.binds.items():
                binds+= f' --bind {key}:{value}'
            command = f"singularity exec --nv --writable-tmpfs {binds} {image} bash {entrypoint}"
        else:
            command = f"bash {entrypoint}"
        command = command.replace('  ',' ') 

        envs = {}
        envs["JOB_ID"]               = f"{job_id}"
        envs["JOB_WORKAREA"]         = workarea 
        envs["TF_CPP_MIN_LOG_LEVEL"] = "3"
        envs["CUDA_VISIBLE_ORDER"]   = "PCI_BUS_ID"
        envs["CUDA_VISIBLE_DEVICES"] = os.environ.get("CUDA_VISIBLE_DEVICES","-1")
        envs["OMP_NUM_THREADS"]      = os.environ.get("SLURM_CPUS_PER_TASK", '4')
        envs["SLURM_CPUS_PER_TASK"]  = envs["OMP_NUM_THREADS"]
        envs["SLURM_MEM_PER_NODE"]   = os.environ.get("SLURM_MEM_PER_NODE", '2048')
        envs.update(job.envs)
        pprint(envs)
        logger.info("ðŸš€ run job!")   
        logger.info(f"command: {command}")
        
        logger.info("starting the process...")
        proc = Popen(command, envs = envs)
        logger.info("process started.")
        proc.run_async()
        
        logger.info("updating job status to running...")
        job.status = State.RUNNING
        while proc.is_alive():
            sleep(10)
            job.ping()
    except:
        traceback.print_exc()
        logger.error("error during the job execution.")
        job.status=State.FAILED
        sys.exit(0)

   
    
    logger.info("job execution completed.")
    if proc.status()!="completed":
        logger.error(f"something happing during the job execution. exiting with status {proc.status()}")
        job.status=State.FAILED
        sys.exit(0)
    
    
    logger.info("uploading output files into the storage...")
    for filename, targetpath in outputs:
        logger.info(f"uploading output file {filename} to storage location {targetpath}...")
        if os.path.exists(filename):
            shutil.move( filename, targetpath)
            symlink( targetpath, filename )
        else:
            logger.error(f"output file {filename} not found in workarea {workarea}.")
            job.status=State.FAILED
            sys.exit(0)
            
    logger.info("job completed successfully.")
    job.ping()
    job.status=State.COMPLETED
    sys.exit(0)




#
# args 
#
def job_parser():
    parser = argparse.ArgumentParser(description = '', add_help = False)

    parser.add_argument('-i','--input', action='store', dest='input', required = True,
                        help = "The job input file")
    parser.add_argument('-o','--output', action='store', dest='output', required = False, default='circuit.json',
                        help = "The job output")
    parser.add_argument('-m','--message-level', action='store', dest='message_level', required = False, default='INFO',
                        help = "The job message level (DEBUG, INFO, WARNING, ERROR)")

    return [parser]
    
    

